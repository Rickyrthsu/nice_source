import os
import json
import requests
import sys
import shutil 
from pathlib import Path 
from urllib.parse import urlparse, parse_qs

def scrape_anime(cvalue):
    print(f"--- [æ‰‹å‹•æ¨¡å¼å•Ÿå‹•] ---")
    
    # é æœŸæ ¼å¼: å½±ç‰‡é€£çµ , æ¨™é¡Œ , åœ–ç‰‡é€£çµ
    try:
        if "," not in cvalue:
            print("âŒ éŒ¯èª¤ï¼šè«‹ä½¿ç”¨ã€å½±ç‰‡é€£çµ , æ¨™é¡Œ , åœ–ç‰‡é€£çµã€æ ¼å¼è¼¸å…¥ï¼")
            return None
            
        parts = [p.strip() for p in cvalue.split(',')]
        if len(parts) < 3:
            print("âŒ éŒ¯èª¤ï¼šè³‡æ–™ä¸è¶³ï¼Œè«‹ç¢ºä¿æœ‰å…©å€‹ã€,ã€åˆ†éš”ç¬¦è™Ÿã€‚")
            return None
        
        target_url = parts[0]
        title = parts[1]
        external_image_url = parts[2]
        
        print(f"ğŸ“¡ æ¥æ”¶åˆ°æ‰‹å‹•è³‡æ–™ï¼š")
        print(f"   - æ¨™é¡Œ: {title}")
        print(f"   - ç¶²å€: {target_url}")
        print(f"   - åœ–ç‰‡: {external_image_url}")

        # 1. è™•ç†åœ–ç‰‡ä¸‹è¼‰è·¯å¾‘
        images_dir = Path('images')
        images_dir.mkdir(exist_ok=True)
        
        # å˜—è©¦å¾ç¶²å€æå– v= ID ä½œç‚ºæª”åï¼Œå¤±æ•—å°±ç”¨æ¨™é¡Œ
        parsed_url = urlparse(target_url)
        video_id_list = parse_qs(parsed_url.query).get('v') 
        video_id = video_id_list[0] if video_id_list else "manual_" + title[:10]
        
        # å¼·åˆ¶å­˜æˆ webp
        image_filename = f"anime_{video_id}.webp"
        internal_image_path = images_dir / image_filename
        
        # 2. ä¸‹è¼‰åœ–ç‰‡ (å¸¶ä¸Š Referer é¿é–‹ç°¡å–®çš„åœ–ç‰‡é˜²ç›œé€£)
        print(f"ğŸ’¾ æ­£åœ¨ä¸‹è¼‰å°é¢åœ–...")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Referer': 'https://hanime1.me/'
            }
            r = requests.get(external_image_url, headers=headers, stream=True, timeout=15)
            r.raise_for_status()
            
            with open(internal_image_path, 'wb') as f:
                shutil.copyfileobj(r.raw, f)
            print(f"âœ¨ åœ–ç‰‡å·²æˆåŠŸå„²å­˜ï¼š{internal_image_path}")
            final_img_path = str(internal_image_path)
        except Exception as img_e:
            print(f"âš ï¸ åœ–ç‰‡ä¸‹è¼‰å¤±æ•—: {img_e}ï¼Œå›é€€ä½¿ç”¨åŸå§‹ç¶²å€ã€‚")
            final_img_path = external_image_url

        # 3. å›å‚³è³‡æ–™çµæ§‹
        return {
            "title": title,
            "imageUrl": final_img_path, 
            "targetUrl": target_url,
            "tags": ["manual_add"], 
            "details": {}
        }

    except Exception as e:
        print(f"âŒ è§£æå¤±æ•—: {e}")
        return None

def main():
    ctype = os.environ.get('COLLECTION_TYPE')
    cvalue = os.environ.get('COLLECTION_VALUE')
    
    if not ctype or not cvalue:
        print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥å€¼")
        sys.exit(1) 

    print(f"--- [GitHub Actions åŸ·è¡Œä¸­] ---")
    new_entry = scrape_anime(cvalue)
    
    if not new_entry:
        sys.exit(1) 
        
    category_map = { 'æ¼«ç•«': 'comic', 'å½±ç‰‡': 'video', 'å‹•æ¼«': 'anime' }
    new_entry['category'] = category_map.get(ctype, 'unknown')

    data_file = 'data.json'
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        data = []
    
    data.insert(0, new_entry)
    
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… æˆåŠŸæ–°å¢è³‡æ–™åˆ° data.jsonï¼")

if __name__ == "__main__":
    main()